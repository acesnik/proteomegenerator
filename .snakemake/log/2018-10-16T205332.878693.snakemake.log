Building DAG of jobs...
Using shell: /bin/bash
Provided cluster nodes: 50
Job counts:
	count	jobs
	1	BuildBamIndex
	2	LongOrfs
	2	Predict
	1	STAR_GTF
	1	StringTie_GTF
	1	UCSC_GTF
	1	all
	2	blastp
	2	cdna_alignment_orf_to_genome_orf
	1	filter
	1	g2g_VCI
	1	g2g_patch
	2	gff3_file_to_bed
	2	gff3_file_to_proteins
	2	gtf_file_to_cDNA_seqs
	2	gtf_to_alignment_gff3
	1	index
	2	maxQuant
	1	merge
	2	mqpar_conversion
	2	reorderFASTA
	32

rule g2g_VCI:
    input: out/variants/K052_tumor_m2_filtered_snps.vcf.gz
    output: out/variants/K052_tumor.vci.gz
    jobid: 25
    benchmark: out/benchmarks/variants/K052_tumor_g2g_VCI.txt
    wildcards: RG_TUMOR=K052_tumor

Submitted job 25 with external jobid 'Job <4869484> is submitted to default queue <cpuqueue>.'.
Finished job 25.
1 of 32 steps (3%) done

rule g2g_patch:
    input: out/variants/K052_tumor.vci.gz
    output: out/custom_ref/K052_tumor_SNP_patched.fa
    jobid: 18
    benchmark: out/benchmarks/variants/K052_tumor_g2g_patch.txt
    wildcards: RG_TUMOR=K052_tumor

Submitted job 18 with external jobid 'Job <4869594> is submitted to default queue <cpuqueue>.'.
Finished job 18.
2 of 32 steps (6%) done
Skipped removing non-empty directory /lila/data/kentsis/Databases/STAR/

rule index:
    input: out/custom_ref/K052_tumor_SNP_patched.fa
    output: /lila/data/kentsis/Databases/STAR/
    log: out/logs/index.txt
    jobid: 36
    benchmark: out/benchmarks/index.txt

Submitted job 36 with external jobid 'Job <4869595> is submitted to default queue <cpuqueue>.'.
Terminating processes on user request, this might take some time.
Will exit after finishing currently running jobs.
Cancelling snakemake on user request.
