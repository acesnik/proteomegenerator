Building DAG of jobs...
Using shell: /bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	all
	1	mqpar_conversion
	2

rule mqpar_conversion:
    input: /lila/data/kentsis/testfiles/K0562_testing/out/all-merge/merged/proteome.fasta, /lila/home/chenz4/proteomegenerator/scripts/mqpar_dummy.xml, /lila/data/kentsis/Raw_files/K056/160116_K052_OffLRP_RP_f01.raw, /lila/data/kentsis/Raw_files/K056/160116_K052_OffLRP_RP_f02.raw
    output: mqpar.xml
    log: merged.mqpar_conversion.txt
    jobid: 1
    benchmark: merged.mqpar_conversion.txt

    Error in rule mqpar_conversion:
        jobid: 1
        output: mqpar.xml
        log: merged.mqpar_conversion.txt

RuleException:
CalledProcessError in line 8 of /lila/home/chenz4/proteomegenerator/pgm2:
Command ' set -euo pipefail;  /home/chenz4/miniconda3/bin/python /lila/home/chenz4/proteomegenerator/scripts/.snakemake.2pxqct70.mqparconverter.py ' returned non-zero exit status 1.
  File "/lila/home/chenz4/proteomegenerator/pgm2", line 8, in __rule_mqpar_conversion
  File "/home/chenz4/miniconda3/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /lila/home/chenz4/proteomegenerator/.snakemake/log/2018-06-20T164551.038288.snakemake.log
