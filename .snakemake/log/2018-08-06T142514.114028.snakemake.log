Building DAG of jobs...
Using shell: /bin/bash
Provided cluster nodes: 50
Job counts:
	count	jobs
	1	BuildBamIndex
	2	LongOrfs
	2	Predict
	1	STAR_GTF
	1	StringTie_GTF
	1	UCSC_GTF
	1	all
	2	blastp
	2	cdna_alignment_orf_to_genome_orf
	1	filter
	2	gff3_file_to_proteins
	2	gtf_file_to_cDNA_seqs
	2	gtf_to_alignment_gff3
	2	maxQuant
	1	merge
	2	mqpar_conversion
	2	reorderFASTA
	27

rule STAR_GTF:
    input: /data/poirier/indexes/GRCh38/STAR
    output: out/FCH9EFLADXX-HUMbghEAACRAAPEI-225.Aligned.sortedByCoord.out.bam
    log: out/logs/FCH9EFLADXX-HUMbghEAACRAAPEI-225.align.txt
    jobid: 27
    benchmark: out/benchmarks/FCH9EFLADXX-HUMbghEAACRAAPEI-225.align.json
    wildcards: sample=FCH9EFLADXX-HUMbghEAACRAAPEI-225

Submitted job 27 with external jobid 'Job <4384129> is submitted to default queue <cpuqueue>.'.
    Error in rule STAR_GTF:
        jobid: 27
        output: out/FCH9EFLADXX-HUMbghEAACRAAPEI-225.Aligned.sortedByCoord.out.bam
        log: out/logs/FCH9EFLADXX-HUMbghEAACRAAPEI-225.align.txt
        conda-env: /lila/data/kentsis/testfiles/K0562_newtest/.snakemake/conda/1a8bc40b

Job failed, going on with independent jobs.
Exiting because a job execution failed. Look above for error message
Complete log: /lila/home/chenz4/proteomegenerator/.snakemake/log/2018-08-06T142514.114028.snakemake.log
