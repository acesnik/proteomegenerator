Building DAG of jobs...
Using shell: /bin/bash
Provided cluster nodes: 50
Job counts:
	count	jobs
	1	AlignFastqReadsByRG
	1	ApplyBQSR
	1	BaseRecalibrator
	1	MarkDuplicates
	1	MergeUnmappedAndAlignedBAMs
	1	SortAndFixRealignedBamTags
	1	UnalignFastqReadsByRG
	1	all
	8

rule UnalignFastqReadsByRG:
    input: /lila/data/kentsis/testfiles/K052M2_test/k052_unaligned_r1.fastq, /lila/data/kentsis/testfiles/K052M2_test/k052_unaligned_r2.fastq
    output: out/K052_tumor}.unaligned.bam
    jobid: 6
    benchmark: out/benchmarks/K052_tumor}.unaligned.txt
    wildcards: SAMPLE_NAME=K052_tumor}

Submitted job 6 with external jobid 'Job <4280966> is submitted to default queue <cpuqueue>.'.

rule AlignFastqReadsByRG:
    input: /lila/data/kentsis/testfiles/K052M2_test/k052_unaligned_r1.fastq, /lila/data/kentsis/testfiles/K052M2_test/k052_unaligned_r2.fastq
    output: out/K052_tumor}.aligned.bam
    jobid: 7
    benchmark: out/benchmarks/K052_tumor}.align.txt
    wildcards: SAMPLE_NAME=K052_tumor}

Submitted job 7 with external jobid 'Job <4280967> is submitted to default queue <cpuqueue>.'.
Terminating processes on user request, this might take some time.
Will exit after finishing currently running jobs.
Cancelling snakemake on user request.
